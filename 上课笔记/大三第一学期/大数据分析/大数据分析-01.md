## Hadoop

### MapReduce

- 伸缩性：**线性** ？？？qq
- 适用于 `一次写入，多次读出` 的批处理业务
- 使用动态模式
- MapReduce更适合批量更新大批量的数据集

### GFS 

Google File System

https://www.cnblogs.com/xybaby/p/8967424.html （blog）

![img](https://images2018.cnblogs.com/blog/1089769/201805/1089769-20180501090649479-435901512.png)

可以看见 GFS系统由三部分组成：**GFS master、GFS Client、GFS chunkserver**。其中，GFS master任意时刻只有<u>一个</u>，而chunkserver和gfs client可能有多个。

一份文件被分为多个固定大小的chunk（**64M**），每个chunk有全局唯一的文件句柄 －－ 一个**64**位的chunk ID，每一份chunk会被复制到多个chunkserver（默认值是**3**)，以此保证可用性与可靠性。

chunkserver将chunk当做**普通的Linux文件**存储在本地磁盘上。

- GFS master
  - 是系统的元数据服务器，维护的元数据包括：命名空间（GFS按层级目录管理文件）、文件到chunk的映射，chunk的位置。其中，前两者是会持久化的，而chunk的位置信息来自于**Chunkserver的汇报**。
  - GFS master还负责分布式系统的**集中调度**：chunk lease管理，垃圾回收，chunk迁移等重要的系统控制。master与chunkserver保持常规的心跳，以确定chunkserver的状态。
- GFS client
  - 是给应用使用的API，这些API接口与POSIX API类似。GFS Client会**缓存**从GFS master读取的chunk信息（即元数据），尽量减少与GFS master的交互。 （异步写）

#### GFS简介

- 系统是构建在普通的、廉价的机器上，因此**故障是常态**而不是意外
- 系统希望存储的是大量的大型文件（单个文件size很大）
- 读——系统支持两种类型读操作：**大量的顺序读取以及小规模的随机读取**（large streaming reads and small random reads.）
- 写——系统的写操作主要是**顺序的追加写**，而不是覆盖写
- 系统对于**大量客户端并发的追加写**有大量的优化，以保证写入的高效性与一致性，主要归功于原子操作record append
- 系统更看重的是**持续稳定的带宽**而不是单次读写的延迟

#### GFS文件读操作流程

- 应用程序调用GFS client提供的接口，表明要读取的文件名、偏移、长度。
- GFS Client将偏移按照规则翻译成chunk序号，发送给master
- master找到对应的chunk句柄和副本位置，回复给客户端. 客户端进行缓存之后发送请求
- GFS client向**最近的**持有副本的Chunkserver发出读请求，请求中包含chunk id与范围
- ChunkServer读取相应的文件，然后将文件内容发给GFS client。

#### GFS文件写操作流程

-  Client向master请求Chunk的副本信息，以及哪个副本（Replica）是Primary

- maste回复client，client**缓存**这些信息在本地

  ```
  之后写过程中，会发送缓存的版本信息，来解决缓存的不一致问题
  ```

- client将数据（Data）链式推送到所有副本

  ```
  目的：最大化利用每个机器的网络带宽，避免网络瓶颈和高延迟连接，最小化推送延迟
  ```

- Client通知Primary提交

- primary在自己成功提交后，通知所有Secondary提交

- Secondary向Primary回复提交结果

- primary回复client提交结果

#### Check Sum

checksum，一个chunk被分解为多个64KB的块，每个块有对应32位的checksum。checksum被保存在内存中，并用利用日志持久化保存，与用户数据是隔离的，当然，这里的内存和持久化都是在chunkserver上。当chunkserver收到读数据请求的时候，会比对文件数据与对应的checksum，如果发现不匹配，会告知client，client从其他的读取；同时，也会告知master，master选择新的chunkserver来restore这个损坏的chunk



![img](https://images2018.cnblogs.com/blog/1089769/201805/1089769-20180501092046288-1923656632.png)

#### 元数据

master主要存储三种类型的元数据：

- 文件和chunk的命名空间

- 从文件到chunk的映射

- 每个chunk副本的位置 (不由master持久化)

  所有的元数据被保存在master的内存中。前两种也会持久化保存，通过记录操作日志，存储在master的本地磁盘并且复制到远程机器。使用操作日志允许我们更简单可靠的更新master状态，不会因为master的当机导致数据不一致。master不会持久化存储chunk位置，相反，master会在启动时询问每个chunkserver以获取它们各自的chunk位置信息，新chunkserver加入集群时也是如此。

#### 租约机制

```
GFS系统中通过租约（lease）机制将chunk写操作授权给ChunkServer。拥有租约授权的ChunkServe称为主ChunkServer，其他副本所在的ChunkServer称为备ChunkServer。租约授权针对单个chunk，在租约有效期内，对该chunk的写操作都由主ChunkServer负责，从而减轻Master的负载。一般来说，租约的有效期比较长，比如60秒，只要没有出现异常，主ChunkServer可以不断向Master请求延长租约的有效期直到整个chunk写满。
```

#### 一致性模型 (有点麻烦)

```
多个writer做append和overwrite操作，record次序和指定region的最终更新取决于leader的排序。所以是不确定的，但chunk的副本是一致的
write: consistent but undefined
append: defined

GFS 和 HDFS 的写入流程都采用了流水线方式，但 HDFS 没有分离数据流和控制流。 
HDFS 的数据流水线写入在网络上的传输顺序与最终写入文件的顺序一致。 
而 GFS 数据在网络上的传输顺序与最终写入文件的顺序可能不一致。 
GFS 在支持并发写入和优化网络数据传输方面做出了最佳的折衷。
```

自己的看法：多副本的分布式文件系统非常类似于 RAID 和缓存—磁盘的机制，一个非常棘手的点在于保证数据的一致性，也就是在数据更改的时候，保证不同的副本内容一致。我们先来看看大牛的架构思路。



各种数据变异在不断发生，被它们改变的文件区域处于什么状态？这取决于变异是否成功了、有没有并发变异等各种因素。

- 对于文件区域A，如果所有客户端从任何副本上读到的数据都是相同的，那A就是一致(**consisitent**)的。

- 如果A是一致的，并且客户端可以看到变异写入的完整数据，那A就是**defined**。

当一个变异成功了、没有受到**并发写**的干扰，它写入的区域将会是defined（也是一致的）：所有客户端都能看到这个变异写入的完整数据。对同个区域的多个并发变异成功写入，此区域是一致的，但可能是undefined：所有客户端看到相同的数据，但是它可能不会反应任何一个变异写的东西，可能是多个变异混杂的碎片。一个失败的变异导致区域不一致（也是undefined）：不同客户端可能看到不同的数据在不同的时间点。下面描述我们的应用程序如何区分defined区域和undefined区域。

　　数据变异可能是写操作或者record append。写操作导致数据被写入一个用户指定的文件偏移。而record append导致数据（record）被**原子**的写入GFS选择的某个偏移（正常情况下是文件末尾，见章节3.3），GFS选择的偏移被返回给客户端，其代表了此record所在的defined区域的起始偏移量。另外，某些异常情况可能会导致GFS在区域之间插入了padding或者重复的record。他们占据的区域可认为是不一致的，不过数据量不大。

　　如果一系列变异都成功写入了，GFS保证发生变异的文件区域是defined的，并完整的包含最后一个变异。GFS通过两点来实现：

(a) chunk的所有副本按相同的顺序来实施变异（章节3.1）

(b) 使用chunk版本数来侦测任何旧副本，副本变旧可能是因为它发生过故障、错过了变异（章节4.5）。执行变异过程时将跳过旧的副本，客户端调用master获取chunk位置时也不会返回旧副本。GFS会尽早的通过垃圾回收处理掉旧的副本。

　　因为客户端缓存了chunk位置，所以它们可能向旧副本发起读请求。不过缓存项有**超时机制**，文件重新打开时也会更新。而且，我们大部分的文件是append-only的，这种情况下旧副本最坏只是无法返回数据（append-only意味着只增不减也不改，版本旧只意味着会丢数据、少数据），而不会返回过期的、错误的数据。一旦客户端与master联系，它将立刻得到最新的chunk位置（不包含旧副本）。

　　在一个变异成功写入很久之后，组件的故障仍然可能腐化、破坏数据。GFS中，master和所有chunkserver之间会持续handshake通讯并交换信息，借此master可以识别故障的chunkserver并且通过检查checksum侦测数据腐化（章节5.2）。一旦发现此问题，会尽快执行一个restore，从合法的副本复制合法数据替代腐化副本（章节4.3）。一个chunk也可能发生不可逆的丢失，那就是在GFS反应过来采取措施之前，所有副本都被丢失。通常GFS在分钟内就能反应。即使出现这种天灾，chunk也只是变得不可用，而不会腐化：应用收到清晰的错误而不是错误的数据。

### Hadoop生态圈

![img](https://img-blog.csdn.net/20160921085138825?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

- 􏰳  **Avro**用于数据序列化的系统;
- 􏰳  **HDFS**是一种<u>分布式文件</u>系统，运行于大型商用机集群，HDFS为HBase提供了高可靠性的底层存储支持; （**Hadoop分布式文件系统**）
- 􏰳  **HBase**位于结构化存储层，是一个**分布式的列存储数据库**;
- 􏰳  **MapReduce**是一种<u>分布式数据处理模式和执行环境</u>，为HBase提供了高性能的计算能力; （**分布式计算框架**）
- 􏰳  **Zookeeper**是一个分布式的、高可用性的**协调服务**，提供分布式锁之类的基本服务，用于构建分布式应用，为HBase提供了稳定服务和failover机制;（**分布式协作服务**）
- 􏰳  **Hive**是一个建立在Hadoop 基础之上的**数据仓库**，它提供了一些用于<u>数据整理、特殊查询和分析</u>存储在Hadoop 文件中的数据集的工具;
- 􏰳  **Pig**是一种数据流语言和运行环境，用以检索非常大的数据集，大大简化了Hadoop常见的工作任务;
- 􏰳  **Sqoop**为HBase提供了方便的RDBMS数据导入功能，使得传统数据库数据向HBase中迁移变的非常方便。
-   **yarn** 分布式资源管理器

### Hadoop环境搭建

#### 运行模式

- 独立式
  - 默认模式。
  - 不对配置文件进行修改。
  - 使用**本地文件**系统，而不是分布式文件系统。
  - Hadoop不会启动NameNode、DataNode、JobTracker、TaskTracker等守护进程，Map()和Reduce()任务作为同一个进程的不同部分来执行的。
  - 用于对MapReduce程序的逻辑进行**调试**，确保程序的正确。

- 伪分布式
  - 在**一台主机**模拟多主机。
  - Hadoop启动NameNode、DataNode、JobTracker、TaskTracker这些守护进程都在同一台机器上运行，是相互独立的Java进程。
  - 在这种模式下，Hadoop使用的是**分布式文件系统**，各个作业也是由JobTraker服务，来管理的独立进程。在单机模式之上增加了代码调试功能，允许检查内存使用情况，HDFS输入输出，以及其他的守护进程交互。类似于完全分布式模式，因此，这种模式常用来**开发测试**Hadoop程序的执行是否正确。
  - 修改3个配置文件：core-site.xml（Hadoop集群的特性，作用于全部进程及客户端）、hdfs-site.xml（配置HDFS集群的工作属性）、mapred-site.xml（配置MapReduce集群的属性）
  - 格式化文件系统
- 全分布式
  - -Hadoop的守护进程运行在由多台主机搭建的集群上，是**真正的生产环境**。
  - 在所有的主机上安装JDK和Hadoop，组成相互连通的网络。
  - 在主机间设置**SSH免密码登录**，把各从节点生成的公钥添加到主节点的信任列表。
  - 修改3个配置文件：core-site.xml、hdfs-site.xml、mapred-site.xml，指定NameNode和JobTraker的位置和端口，设置文件的副本等参数
  - 格式化文件系统

#### 三个配置文件的解释

https://blog.csdn.net/learn_tech/article/details/81531248

#### HDFS上的数据存储操作

- 􏰆􏰇􏰈􏰉􏰂􏰈􏰊􏰋适合大量的大文件
- 平均单个文件超过**500M** (重要)
- 一次写入，多次读出
- 单个文件内容不可以被修改，除非在末尾添加新的内容
- 可以：
  - 创建文件
  - 向文件末尾添加内容
  - 删除文件
  - 修改文件名、文件属性(如：拥有者)

#### 并行计算

1. 数据分布式存储

2. 分布式并行计算

3. 本地计算

   数据在哪里存储，就在哪里进行计算；减少网络的开销

4. 划分任务粒度

5. 数据分割 (partition)

6. 数据合并 (combine)

   是Map内的一步，减少网络的开销

7. Reduce

8. 任务管道 -> 流水线式的计算，上一步的Output可以作为下一步的Input

NameNode 

JobTrunk 