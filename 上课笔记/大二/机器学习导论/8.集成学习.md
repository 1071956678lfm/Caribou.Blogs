## 集成学习

```
Ensemble Learning 
用于构建并结合多个学习器来完成学习任务
```

### 个体与集成

假定基分类器错误率为 $\epsilon_i$ ,那么有
$$
P(h_i(x)\ne f(x))=\epsilon
$$
对于投票表决，如果有超过半数的基分类器正确，那么集成分类的判定为正确
$$
H(x)=sign(\sum_{i=1}^Th_i(x))
$$

#### 集成学习分类

| 类别   | 描述                           | 举例                  |
| ------ | ------------------------------ | --------------------- |
| 串行化 | 个体学习器之间存在强依赖关系   | AdaBoost              |
| 并行化 | 个体学习器之间不存在强依赖关系 | Bagging <br/>随机森林 |

### Boosting

#### 加性模型

```
基学习器的线性组合
```

$$
H(x)=\sum_{t=1}^T\alpha_th_t(x)
$$


##### 最小化目标

指数损失函数
$$
l_{exp}(H|D)=E_{x\sim D}[e^{-f(x)H(x)}]
$$

#### 算法流程

输入训练集 $T=\{(x_1,y_1),(x_2,y_2),...,(x_m,y_m)\}$

基学习器 $clf$

首先需要初始化 $\alpha$ 和 $h(x)$ 的列表，用于最后的加性模型 $H(x)=\sum{\alpha_t h_t(x)}$

1. 初始化每一个样本的权值分布 $D_1(x)=\frac1m$

2. $for \; t\;in\;range(epoches)$

   1. 给出基学习器 $clf$ 的预测值 $y\_pred$

   2. 计算$D_t$权值分布下, 总体的误差率  $\epsilon_t = \frac{\sum_{i=1}^mD_i II(y_i\ne pred)}{\sum_{i=1}^mD_i}$ , 其中 $\sum D_i$与 $1$ 相差很小

   3. 计算新的 $\alpha_t = \frac12\ln{(\frac{1-\epsilon_t}{\epsilon_t})}$

   4. 更新所有的权重 $D_{t+1}=\frac{D_t(x)\exp{(-\alpha_tf(x)h(x))}}{Z_t}$

      其中 $Z_t = \frac{\sum_{i=1}^mD_i\exp(-y\alpha_th(x))}{\sum_{D_i}}$

#### 算法推导



#### 特点

- 关注于降低 **偏差** ，能够基于泛化性能弱的学习器构建出很强的集成

### Bagging算法族

#### 自助采样

```
若需要从样本中得到m个样本，那么有放回地拿取m次，这样的采样方式称作自助采样
```

$$
\lim_{m->infinty}(1-\frac1m)^m=\frac1e
$$

则最终有 $\frac1e$ 的数据不被采样，可以作为 **包外估计 $out-of-bag \;estimate $**。

我们假设 $D_t$ 作为 $h_t$ 使用的训练样本集，令 $H^{oob}(x)$表示对样本 $x$ 的包外预测

那么
$$
H^{oob}(x)=\arg\max_{y\in Y}\sum_{t=1}^TII(h_t(x)=y)\times II(x\notin D_t)
$$

