## 支持向量机

### 间隔与支持向量

##### 定义

给定训练样本集$D=\{(x_1,y_1),(x_2,y_2),...(x_m,y_m)\}$ , $y_i\in \{-1,+1\}$,我们期望找到一个超平面，将其分开

划分超平面 $w^Tx+b=0$

其中：

​	$w=(w_1;w_2;...;w_d)$为法向量

​	$b$为位移项，决定了超平面和远点之间的距离

样本空间中的任意一点$x$和超平面$(w,b)$的距离可以为：
$$
r=\frac{|w^Tx+b|}{||w||}
$$
假设该超平面可以进行正确分类

那么满足
$$
\begin{align}
&w^Tx_i+b\ge +1,&&y_i=+1;\\
& w^Tx+b\ge -1,&&y_i=-1;\;(1)
\end{align}
$$
我们选取离超平面**最近**的点(称为`支持向量`)，使得等号成立

两个异类支持向量到超平面的距离为`间隔`：
$$
\gamma=\frac{2}{||w||}
$$

##### 目的

我们期望找到具有最大间隔的超平面,并且满足(1)的约束

也即：
$$
\max_{w,b}\frac{2}{||w||}\\s.t.y_i(w^T_ix_i+b)\ge 1
$$
我们得到其等价的优化：最大化$||w^{-1}||$---最小化$||w^2||$

得到支持向量机的基本型：
$$
\min_{w,b}\frac{||w^2||}{2}\\s.t.y_i(w^T_ix_i+b)\ge 1\;\;i=1,2...m\;\;\;(2)
$$
显然这是一个 `在限制条件下的凸优化`

### 对偶问题

##### 推导

采用拉格朗日乘子法,对(2)的每一条约束添加拉格朗日乘子$\alpha_i \ge 0$ 

```
拉格朗日乘子法中，对于约束条件 G(x) < 0 , 则可以给予拉格朗日乘子 lambda
```

得到对应的拉格朗日函数
$$
\begin{align}
L(w,b,\alpha)=\frac12||w^2||+\sum_{i=1}^m\alpha_i(1-y_i(w^Tx_i+b)&&(3)
\end{align}
$$
其中$\alpha=(\alpha_1;\alpha_2;...;\alpha_m)$
$$
\begin{align}
L(w,b,\alpha)
&=\frac12w^Tw+\sum_{i=1}^m\alpha_i(1-y_i(w^Tx_i+b))\\
&=\frac12w^Tw+\sum_{i=1}^m\alpha_i-\sum_{i=1}^m\alpha_iy_iw^Tx_i-\sum_{i=1}^m\alpha_iy_ib
\end{align}
$$


拉格朗日函数对 **未知参数** $w , b$求偏导,取0，得

$\frac{\partial{L}}{\partial{w}}=\frac12\times 2w+0-\sum_{i=1}^m\alpha_iy_ix_i =0$
$$
\begin{align}
w&=\sum_{i=1}^m\alpha_iy_ix_i&&(4)\\
0&=\sum_{i=1}^m\alpha_iy_i&&(5)
\end{align}
$$
将(4)代入(3)，得到关于(5)的约束下的最优化对偶问题
$$
\begin{align}
\min_{w,b}L(w,b,\alpha)
&=\frac12w^Tw+\sum_{i=1}^m\alpha_i-\sum_{i=1}^m\alpha_iy_iw^Tx_i-\sum_{i=1}^m\alpha_iy_ib\\
&=\frac12w^T\sum_{i=1}^m\alpha_iy_ix_i+\sum_{i=1}^m\alpha_i-w^T\sum_{i=1}^m\alpha_iy_ix_i-b\sum_{i=1}^m\alpha_iy_i\\
&=-\frac12w^T\sum_{i=1}^m\alpha_iy_ix_i+\sum_{i=1}^m\alpha_i-0\\
&=-\frac12(\sum_{i=1}^m\alpha_iy_ix_i)^T(\sum_{i=1}^m\alpha_iy_ix_i)+\sum_{i=1}^m\alpha_i\\
&=\sum_{i=1}^{m}\alpha_i-\frac12\sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_jy_iy_jx_i^Tx_j
\end{align}
$$
故最终为：

$$
\begin{align}
\max_{\alpha}\min_{w,b}L(w,b,\alpha)&=\max_{\alpha}\sum_{i=1}^{m}\alpha_i-\frac12\sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_jy_iy_jx_i^Tx_j\\
&s.t.\;\;0=\sum_{i=1}^m\alpha_iy_i\\
&\alpha_i\ge 0\;\;\;(6)
\end{align}
$$

如果我们求解出$\alpha$,那么就可以得到$w,b$
$$
f(x)=w^Tx+b=\sum_{i=1}^m\alpha_iy_ix_i^Tx+b\;\;\;(7)
$$

##### KKT条件

对于求得$f(x)$的极值

- 考虑等式约束$g(x)=0$

在最优点$x^*$的梯度$\nabla f(x^*)$和$\nabla g(x^*)$的方向相同或相反

即存在$\lambda$,使得
$$
\nabla f(x^*)+\lambda\nabla g(x^*)=0
$$


- 考虑不等式约束$g(x)\le 0$

此时最优点$x^*$在$g(x)\le 0$的范围中

1. 若$g(x)\lt 0$，此时$g(x)$约束没有必要，那么可以直接通过$\nabla f(x)$ 求得极值 ($\lambda = 0$)
2. 若$g(x)=0$，那么和等式约束类似，不过此时梯度$\nabla f(x^*)$和$\nabla g(x^*)$的方向**相反**

也即存在常数$\lambda \gt 0$使得$\nabla f(x^*)+\lambda\nabla g(x^*)=0$

由上可知，

$g(x)\le 0$

$\lambda \ge 0$

$\lambda g(x) = 0$

以上便是**KTT条件**

------

因为是不等式约束，需要满足如下条件: `KKT条件`

$\alpha_i\ge 0$

$y_if(x_i)-1\ge 0$

$\alpha_i (y_if(x_i)-1)=0$

所以最终为$\alpha_i = 0 $或$y_if(x_i)=1$

- 如果$\alpha_i=0$，样本不会在(7)出现：即不对$f(x)$产生影响
- 如果$y_if(x_i)=1$,那么对应的样本位于最大间隔的边界上：是一个**支持向量**

##### 求解二次规划

接下来考虑到**(6)的求解问题**

###### `SMO`

Sequential Minimal Optimization

基本思路：

- 先固定$\alpha_i$之外的参数，然后求$\alpha_i$的极值——$\sum_iy_i\alpha_i=0$，知道其他的$\alpha_j$，可以知道$\alpha_i $ 

- 每次选择两个参数$\alpha_i,\alpha_j$,然后固定其他参数
- 不断重复如下步骤
  - 选取一对$\alpha_i, \alpha_j$
  - **固定**其他参数，求解**(6)**，获得更新后的$\alpha_i, \alpha_j$

当选取的$\alpha_i,\alpha_j $中**有一个**不满足 `KKT条件`，目标函数就会增大；

1. 选取违背 `KKT条件`程度最大的变量
2. 采用启发式，使得选取的两个变量差别较大

由于采用两个变量，**目标函数(6)**的约束可以重写为
$$
\alpha_iy_i+\alpha_jy_j=c\;\;\;,\alpha_i\ge 0,\alpha_j\ge 0\\
c=\sum_{k\ne i,j}\alpha_ky_k\;\;\;(8)
$$
代入目标函数，消去$\alpha_j$,得到关于$\alpha_i$的单变量二次规划问题。仅有的约束为$\alpha_i\ge 0 $

------

此外，为了求得$b$

对于支持向量$(x_s,y_s)$,有$y_sf(x_s)=1$

则$1=y_s(w^Tx_s+b)=y_s(\sum_{i\in S}\alpha_sy_sx_s^Tx_s+b)$

理论上可以选择任意支持向量来求得$b=\frac1{y_s}-\sum_{i\in S}\alpha_iy_ix_i^Tx_i$，现实中常常用所有支持向量平均值
$$
b=\frac{1}{|S|}\sum_{s\in S}(\frac1{y_s}-\sum_{i\in S}\alpha_iy_ix_i^Tx_i)
$$

### 核函数

以上的问题为**线性可分**，如果问题变为线性不可分，需要

```
将原始空间映射到更高维度的特征空间，使得样本在该特征空间内线性可分
```

##### 原理

将$x$进行映射后的特征向量为$\phi(x)$

对应的模型为
$$
f(x)=w^T\phi(x)+b
$$
类似地，我们得到凸优化目标
$$
\min_{w,b}\frac12||w||\\
s.t.\;y_i(w^T\phi(x)+b)\ge 0
$$
其对偶问题
$$
\begin{align}&
\max_{\alpha}\sum_{i=1}^{m}\alpha_i-\frac12\sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_jy_iy_j\phi(x_i)^T\phi(x_j)\\
&s.t.\;\;0=\sum_{i=1}^m\alpha_iy_i\\
&\alpha_i\ge 0\;\;\;\end{align}
$$
由于计算高维的内积$\Phi(x_i)^T\Phi(x_j)$复杂度较大，设想如下
$$
\kappa(x_i,x_j)=<\phi(x_i),\phi(x_j)>=\phi(x_i)^T\phi(x_j)\;\;\;(9)
$$

```
将高维度的内积转化为原始空间内的函数值 
```

由此，对偶问题转化为
$$
\max_{\alpha}\sum_{i=1}^{m}\alpha_i-\frac12\sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_jy_iy_j\kappa(x_i,x_j)\\
s.t.\;\;0=\sum_{i=1}^m\alpha_iy_i\\
\alpha_i\ge 0\;\;\;(10)
$$

```
我们把(9)称为核函数(kernel fucntion),把(10)称为支持向量展式
```

##### 定理 核函数

- 令$\chi$为输入空间，$\kappa(.,.)$为$\chi\times\chi$上的**对称函数**，那么

  $\kappa(.,.)$为核函数当且仅当：对于任意的数据$D=\{x_1,x_2,...,x_m\}$，核矩阵$K$总是**半正定**的

$$
K=\begin{bmatrix} 
\kappa(x_1,x_1) & \kappa(x_1,x_2) & ...&\kappa(x_1,x_j) & ... &\kappa(x_1,x_m)\\ 
\kappa(x_2,x_1) & \kappa(x_2,x_2) & ...&\kappa(x_2,x_j) & ... &\kappa(x_2,x_m)\\
... & ... & ...&... & ... &...\\
\kappa(x_m,x_1) & \kappa(x_m,x_2) & ...&\kappa(x_m,x_j) & ... &\kappa(x_m,x_m)\\

\end{bmatrix}
$$

对于每一个半正定矩阵，都可以找到与之对应的映射$\phi$；换言之, 任何一个核函数都隐式定义了一个“再生核希尔伯特空间”

##### 常见核函数

| 名称       | 表达式                                                   | 参数                                          |
| ---------- | -------------------------------------------------------- | --------------------------------------------- |
| 线性核     | $\kappa(x_i,x_j)=x^T_ix_j$                               |                                               |
| 多项式核   | $\kappa(x_i,x_j)=(x_i^Tx_j)^{d}$                         | $d\ge 0$为多项式的系数                        |
| 高斯核     | $\kappa(x_i,x_j)=\exp(-\frac{||x_i-x_j||^2}{2\sigma^2})$ | $\sigma\gt 0 $高斯核的带宽                    |
| 拉普拉斯核 | $\kappa(x_i,x_j)=\exp(-\frac{||x_i-x_j||}{\sigma})$      | $\sigma\gt 0 $                                |
| Sigmoid核  | $\kappa(x_i,x_j)=\tanh(\beta x_i^Tx_j+\theta)$           | $\tanh$为双曲函数，$\beta\gt 0 ,\theta \lt０$ |

此外

- 核函数的线性组合 $\gamma_1\kappa_1+\gamma_2\kappa_2$也是核函数
- 核函数的直积$\kappa_1\bigotimes\kappa_2(x,z) =\kappa_1(x,z)\kappa_2(x,z)$也是核函数

- 对于任意函数$g(x)$，$\kappa(x,z)=g(x)\kappa_1(x,z)g(z)$也为核函数

### 软间隔与正则化

#### 软间隔

对于所有的样本，均需要满足约束$y_i(w^Tx_i+b)\ge 1$，这个称为 `硬间隔`

我们允许一些样本可以出错，引入`软间隔`，他们不满足上述约束

根据如上调整，我们的目标为
$$
\min_{w,b}\frac12||w||^2+C\sum_{i=1}^ml_{0/1}(y_i(w^Tx_i+b)-1)
$$
其中 $C \gt 0$为常数，$l_{0/1}$为 **0/1损失函数**
$$
l_{0/1}(z)=
\begin{cases} 
1, & \text {if $z$}\lt 0 \\ 
0, & otherwise
\end{cases}
$$
由于该损失函数非凸，非连续，数学性质不好，我们采用如下`替代损失函数`

| 名称      | 公式                          |
| --------- | ----------------------------- |
| hinge损失 | $\max(0,1-z)$                 |
| 指数损失  | $l_{exp}(z)=\exp(-z)$         |
| 对率损失  | $l_{log}(z)=\log(1+\exp(-z))$ |

- 若采用hinge损失
  $$
  \min_{w,b}\frac12||w||^2+C\sum_{i=1}^m\max(0,1-y_i(w^Tx_i+b))
  $$
  

  那么引入**松弛变量**

$$
\min_{w,b,\xi_i}\frac12||w||^2+C\sum_{i=1}^m\xi_i\\
s.t.\;\;y_i(w^Tx_i+b)\ge1-\xi_i\;,\\
\xi_i\ge 0
$$

此处拥有两个限制，需要加入两个拉格朗日乘子

相似地，通过拉格朗日乘子法
$$
L(w,b,\alpha,\xi,\mu)=\frac12||w||^2+C\sum_{i=1}^m\xi_i+\sum_{i=1}^m\alpha_i(1-\xi_i-y_i(w^Tx_i+b))-\sum_{i=1}^m\mu_i\xi_i
$$
其中 $\alpha,\mu $为拉格朗日乘子

------

对 **未知参数** $w,b,\xi_i$求导
$$
w=\sum_{i=1}^m\alpha_iy_ix_i\\
0=\sum_{i=1}^m\alpha_iy_i\\
C=\alpha_i+\mu_i
$$
代入到$L(w,b,\alpha,\xi,\mu)$推导其对偶问题
$$
\begin{align}
L(w,b,\alpha,\xi,\mu)
&=\frac12||w||^2+C\sum_{i=1}^m\xi_i+\sum_{i=1}^m\alpha_i(1-\xi_i-y_i(w^Tx_i+b))-\sum_{i=1}^m\mu_i\xi_i\\
&=\sum_{i=1}^{m}\alpha_i-\frac12\sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_jy_iy_jx_i^Tx_j\\
s.t.&\sum_{i=1}^m\alpha_iy_i=0,\\
0 &\le \alpha_i \le C
\end{align}
$$

```
它与不加入松弛变量的区别,仅仅在于α的范围上
```

我们考虑其 KKT条件要求

$\alpha_i , \mu_i \ge 0$

$y_if(x_i)-1+\xi_i\ge 0$

$\alpha_i(y_if(x_i)-1+\xi_i)=0$

$\xi_i\ge 0,\;\mu_i\xi_i=0$

- 若$\alpha_i =0$，那么不对$f(x)$有影响
- 若$\alpha_i \ne 0$，有$y_if(x_i)-1+\xi_i= 0$
  - 若 $\alpha_i < C$, 那么$\mu_i > 0$, 故$\xi_i = 0$,此时样本在最大间隔边界
  - 若$\alpha = C$, 那么$\mu_i = 0$
    - 若 $\xi_i \le 1 $则样本落在最大间隔内部
    - 否则，分类错误

#### 正则化

一般性地，优化目标的第一项描述划分超平面的“间隔”大小，另一项表示训练集上的误差
$$
\min_{f}\Omega(f)+C\sum_{i=1}^ml(f(x_i),y_i)
$$
称$\Omega(f)$为**结构风险**，$\sum_{i=1}^ml(f(x_i),y_i)$为**经验风险**，通过 $C$来对二者进行折中

从正则化角度来看

- $\Omega(f)$为正则化项,常常用$L_p$范数代替
- $C$为正则化常数

##### $L_p范数$

- 向量范数

1-范数:
$$
||x||_1=\sum_{i=1}^m|x_i|
$$
2-范数:
$$
||x||_2=\sqrt{\sum_{i=1}^Nx_i^2}
$$
p-范数:
$$
||x||_p=(\sum|x_i|^p)^{\frac1p}
$$

### 支持向量回归

```
传统回归模型模型期望模型输出和label之间差别小最好；支持向量回归(SVR)假设容忍某一个量度的偏差，当且仅当偏差大于这个量度时，才考虑到误差计算中
```

#### 原理

对于回归模型，我们期望
$$
|y_i-f(x_i)|\le \epsilon
$$
即距离尽可能缩短

SVR问题一般形式
$$
\min_{w,b}\frac12||w||^2+C\sum_{i=1}^ml_{\epsilon}(f(x_i)-y_i)
$$
其中$l_{\epsilon}$为$\epsilon$-不敏感损失函数
$$
l_{epsilon}(z)=
\begin{cases} 
0, & \text {if $|z|$}\le \epsilon \\ 
1, & otherwise
\end{cases}
$$
引入松弛变量 $\xi_i $和$\hat{\xi_i}$ (间隔两边的松弛变量设置为不同)
$$
\min_{w,b,\xi_i,\hat{\xi_i}}\frac12||w||^2+C\sum_{i=1}^m(\xi_i+\hat{\xi_i})\\
s.t.\;\;f(x_i)-y_i \le \epsilon + \xi_i\\
y_i-f(x_i) \le \epsilon + \hat{\xi_i}\\
\xi_i , \hat{\xi_i} \ge 0
$$
上述含有四个约束条件，引入四个拉格朗日乘子
$$
L(w,b,\alpha,\hat{\alpha},\xi,\hat{\xi},\mu,\hat{\mu})=\min_{w,b,\xi_i,\hat{\xi_i}}\frac12||w||^2+C\sum_{i=1}^m(\xi_i+\hat{\xi_i})-\sum_{i=1}^m\mu_i\xi_i-\sum_{i=1}^m\hat{\mu_i}\hat{\xi_i}+\\\sum_{i=1}^m\alpha_i(f(x_i)-y_i-\epsilon-\xi_i)+\sum_{i=1}^m\hat{\alpha_i}(y_i-f(x_i)-\epsilon-\hat{\xi_i})
$$
对$w,b,\xi,\hat{\xi}$求偏导，得到SVR的对偶问题

$w=\sum(\hat{\alpha}-\alpha)x_i$

$0=\sum(\hat{\alpha}-\alpha)$

$C=\mu_i+\xi_i$

$C=\hat{\mu_i}+\hat{\xi_i}$

故对偶问题形式如下
$$
\max_{\alpha,\hat{\alpha}}\sum_{i=1}^my_i(\hat{\alpha_i}-\alpha_i)-\epsilon(\alpha_i+\alpha_i)-\frac12\sum_{i=1}^m\sum_{j=1}^m(\hat{\alpha_i}-\alpha_i)(\hat{\alpha_j}-\alpha_j)x^T_ix_j\\
s.t.\;\;\sum(\hat{\alpha_i}-\alpha_i)=0,\\
0 \le \alpha_i,\hat{\alpha_i} \le C
$$
对于以上的KTT条件：
$$
\begin{cases} 
\alpha_i(f(x_i)-y_i-\epsilon-\xi_i)=0\\
\hat{\alpha_i}(y_i-f(x_i)-\epsilon-\xi_i)=0\\
\alpha_i\hat{\alpha_i}=0,\;\xi_i\hat{\xi_i}=0\\
(C-\alpha_i)\xi_i=0,\;(C-\hat{\alpha_i})\hat{\xi_i}=0
\end{cases}
$$

#### 分析

- 仅当样本不落入 $\epsilon$区间内时,相应的拉格朗日乘子$\alpha_i,\hat{\alpha_i}$才能够非零

- $\alpha_i , \hat{\alpha_i}$至少有一个为0

- $f(x)=\sum(\hat{\alpha_i}-{\alpha_i})x^T_ix_i+b$

- 由于$(C-\alpha_i)\xi_i=0\;,\alpha_i(f(x_i)-y_i-\epsilon-\xi_i)=0$, 那么当 $0\lt \alpha_i \lt C$时，必有 $\xi_i = 0$

  进而 $b = y_i + \epsilon - \sum(\hat{\alpha_i}-{\alpha_i})x^T_ix_i$

若考虑特征映射

$f(x)=\sum(\hat{\alpha_i}-{\alpha_i})\kappa(x,x_i)+b$

### 核方法

#### 表示定理

