## 线性模型

### 线性回归

#### 表示形式

###### 在回归任务的情形下，我们需要通过调整线性函数的参数，来尽可能缩小[均方误差]，即欧氏距离

$$
f(\vec x_i)=\vec{w}^T\vec{x_i}+\vec b , 使得f(x_i)\approx y_i
$$

​	称为多元线性回归

为了缩小均方误差，若为一元线性回归
$$
(w^*,b^*)=argmin_{(w,b)}\sum_{i=1}^m(f(x_i)-y_i)^2\\
=argmin_{(w,b)}\sum_{i=1}^m(y_i-wx_i-b_i)^2
$$
通过均方误差求解的过程，称为**最小二乘**

接下来对于多元线性回归：
$$
取如下矩阵\\X=\begin{pmatrix} x_{11} & x_{12} & ... & x_{1d} & 1\\ 
x_{21} & x_{22} & ... & x_{2d} & 1\\ 
. & . & ... & . & .\\ 
. & . & ... & . & .\\ 
x_{m1} & x_{m2} & ... & x_{md} & 1\\ 
\end{pmatrix} 
=\begin{pmatrix}
X_1^T & 1\\
X_2^T & 1\\
. & . \\
. & . \\
X_m^T & 1

\end{pmatrix}\\
令\hat{w}=\begin{bmatrix}w\\b\end{bmatrix},
\hat{x_i}=\begin{bmatrix}x_i\\1\end{bmatrix}
\\那么\,x_i^Tw+b=\hat{x_i}^T\hat{w}
$$

##### 梯度

- 方向导数为各个方向上的导数
- 梯度的方向是方向导数的最大值方向，梯度的值是方向导数的最大值

$$
设f(x,y)为一个二元函数，u=cos\theta_i+sin\theta_j为一个单位向量\\
那么\lim_{t->0}\frac{}{}
$$



##### 矩阵求导

###### 矩阵内积

$$
<X,Y>=X^TY=Tr(X^TY)=\sum_{i=1}^m\sum_{j=1}^nX_{ij}Y_{ij}
$$

### 对数几率回归

###### 虽然是“回归”，但是应用于分类问题

###### 将 R 映射至 [0,1]

对数几率函数(sigmod 函数)
$$
y=\frac{1}{1+e^{-z}}
$$

- 将线性模型代入

$$
y=\frac{1}{1+e^{-{(w^Tx+b)}}}
$$

- 进而转为

$$
ln\frac{y}{1-y}=w^Tx+b
$$

- 其中,y是正例可能性，1-y是反例可能性

$$
\frac{y}{1-y}称为几率\\
ln\frac{y}{1-y}称为对数几率
$$

- 考虑后验概率形式

$$
ln\frac{p(y=1|x)}{p(y=0|x)}=w^Tx+b;\\
$$

​	那么
$$
p(y=1|x)=\frac{e^{w^Tx+b}}{1+e^{w^Tx+b}}\\
p(y=0|x)=\frac{1}{1+e^{w^Tx+b}}
$$

##### 损失函数

$cost = -y_i\log{(p)-(1-y)\log(1-p)}$

##### 极大似然估计

###### 通过若干次试验，观察其结果，利用试验结果得到某个参数值能够使样本出现的概率为最大，则称为极大似然估计

$$
l(\theta)=p(D|\theta)=p(x_1,x_2,...x_N|\theta)=\prod_ip(x_i|\theta)
$$

若θ能够使似然函数最大，那么就令θ为估计值

求解似然估计量步骤

1. 写出似然函数
2. 取对数，得到对数似然函数
3. 求导数
4. 解似然方程

##### 极大似然法求解 w , b 

- 对数似然函数

  ###### 将原来的类乘概率计算转化为累加

$$
l(\theta)=\sum_{i=1}^{m}{\ln p(y_i|x_i;w,b)}
$$

- 进行一些转化
  $$
  let . \;\beta=(w;b),\hat{x}=(x;1)\\
  so . \; w^Tx+b=\beta^Tx\\
  p_1(\hat{x};\beta)=p(y=1|\hat{x};\beta)=\frac{e^{\beta^T\hat{x_i}}}{1+e^{\beta^T\hat{x_i}}}\\
  p_0(\hat{x};\beta)=p(y=0|\hat{x};\beta)=\frac{1}{1+e^{\beta^T\hat{x_i}}}\\
  $$
  那么
  $$
  p(y_i|x_i;w,b)=y_ip_1(\hat{x_i};\beta)+(1-y_i)p_0(\hat{x_i};\beta)\\
  and \; y_i\in \{0,1\}\\
  so . \; \\
  if \;y_i=1,p(y_i|x_i;w,b)=p_1(\hat{x_i};\beta),\\
  y_i=0,p(y_i|x_i;w,b)=p_0(\hat{x_i};\beta)
  $$
  


- $$
  最大化\\
  l(\theta)=\sum_{i=1}^m(y_i\ln{P(y=1|x_i;w)+(1-y_i)\ln{(1-P(y=1|x_i;w))}})
  \\
  =\sum_{i=1}^m(y_i(\ln\frac{e^{\beta^T\hat{x_i}}}{1+e^{\beta^T\hat{x_i}}})+(1-y_i)\ln{\frac{1}{1+e^{\beta^T\hat{x_i}}}})
  \\
  =\sum_{i=1}^m(-y_i\beta^T\hat{x_i}+\ln(1+e^{\beta^T\hat{x_i}}))
  $$

-  ##### ##### 牛顿法,,

### 线性判别分析

###### LDA Linear Discriminant Analysis

###### 通过投影，降低到维度比较低的部分. 将高维的点投影到一条直线，使得点尽量的按类别区分

![img](https://upload-images.jianshu.io/upload_images/53611-32f117094c173b09.gif?imageMogr2/auto-orient/strip%7CimageView2/2/w/328/format/webp)

- 目标：不同类别之间距离远，同一类别距离近

- 区分二分类的直线
  $$
  y=w^Tx
  $$

- 类别 i 的中心点(均值向量)
  $$
  m_i=\frac1m(\sum_{x\in{D_i}} x)
  $$

- 投影后的中心点
  $$
  \widetilde{m_i}=w^Tm_i
  $$

- i 投影之后，类别之间的分散程度（方差）

$$
\widetilde{s_i}=\sum_{y\in{Y_i}}(y-\widetilde{m_i})^2
$$

### 多分类学习

- `OvO` 一对一

1. 如果有n个类别，那么对他们两两配对，形成 n(n-1)/2 种配对方式，进行这么多次的二分类任务
2. 对于一个样本，看所有分类器中投票数最高者，即为其预测值

- `OvR` 一对多

1. 训练时，该标签为正例，其余均为反例。按照这个规律训练n个分类器
2. 最终测试时，看所有分类器中得分最高者，即为其预测值

- `MvM` 多对多

1. 若干个类作为正类，其他的位反类
2. 采用`纠错输出码`

### 类别不平衡问题

##### 背景

有时正例、反例的比例不一致，导致最终训练结果不平衡

##### 分类器决策规则

$$
if \;\frac{y}{1-y}>1\;,then\;Positive
$$

现在我们令
$$
m^+为正例个数，m^-为反例个数\\
那么观测几率为\frac{m^+}{m^-}\\
if \;\frac{y}{1-y}>\frac{m^+}{m^-}\;,then\;Positive
$$

##### 再缩放

$$
let.\;\frac{y'}{1-y'}=\frac{y}{1-y}\times\frac{m^+}{m^-}
$$

但是有时`观测几率不能推知真实几率`

有以下三种做法：

1. 欠采样：去除一些反例来让正反例比例接近
2. 过采样：增加正例比例
3. 阈值移动