## 线性模型

### 线性回归

#### 表示形式

###### 在回归任务的情形下，我们需要通过调整线性函数的参数，来尽可能缩小[均方误差]，即欧氏距离

$$
f(\vec x_i)=\vec{w}^T\vec{x_i}+\vec b , 使得f(x_i)\approx y_i
$$

​	称为多元线性回归

为了缩小均方误差，若为一元线性回归
$$
(w^*,b^*)=argmin_{(w,b)}\sum_{i=1}^m(f(x_i)-y_i)^2\\
=argmin_{(w,b)}\sum_{i=1}^m(y_i-wx_i-b_i)^2
$$
通过均方误差求解的过程，称为**最小二乘**

接下来对于多元线性回归：
$$
取如下矩阵\\X=\begin{pmatrix} x_{11} & x_{12} & ... & x_{1d} & 1\\ 
x_{21} & x_{22} & ... & x_{2d} & 1\\ 
. & . & ... & . & .\\ 
. & . & ... & . & .\\ 
x_{m1} & x_{m2} & ... & x_{md} & 1\\ 
\end{pmatrix} 
=\begin{pmatrix}
X_1^T & 1\\
X_2^T & 1\\
. & . \\
. & . \\
X_m^T & 1

\end{pmatrix}\\
令\hat{w}=\begin{bmatrix}w\\b\end{bmatrix},
\hat{x_i}=\begin{bmatrix}x_i\\1\end{bmatrix}
\\那么\,x_i^Tw+b=\hat{x_i}^T\hat{w}
$$

##### 梯度

- 方向导数为各个方向上的导数
- 梯度的方向是方向导数的最大值方向，梯度的值是方向导数的最大值

$$
设f(x,y)为一个二元函数，u=cos\theta_i+sin\theta_j为一个单位向量\\
那么\lim_{t->0}\frac{}{}
$$



##### 矩阵求导

###### 矩阵内积

$$
<X,Y>=X^TY=Tr(X^TY)=\sum_{i=1}^m\sum_{j=1}^nX_{ij}Y_{ij}
$$

###### 最小二乘法优化

$$
E_{w}=\arg\min(y-X\hat{w})^T(y-X\hat{w})
$$

关于 $\hat{w}$ 求导
$$
\frac{\partial{E}}{\partial{\hat{w}}}=\frac{y^Ty-\hat{w}^TX^Ty-y^TX\hat{w}+\hat{w}^TX^TX\hat{w}}{\partial\hat{w}}\\
=-0-X^Ty-X^Ty+2(X^TX)\hat{w}\\
=2X^T(X\hat w-y)
$$
令 $2X^T(X\hat w - y)=0$时，$w=(X^TX)^{-1}X^Ty$

###### 矩阵求导计算

$$
df=tr(\frac{\partial f^T}{\partial w}dw)
$$

其中，矩阵的迹满足

- $tr(A)=tr(A^T)$
- $tr(AB)=tr(BA)$

矩阵微分满足

- $d(X^T)=(d(X))^T$
- $d(AB)=Ad(B)+d(A)B$

### 对数几率回归

###### 虽然是“回归”，但是应用于分类问题

###### 将 R 映射至 [0,1]

对数几率函数(sigmod 函数)
$$
y=\frac{1}{1+e^{-z}}
$$

- 将线性模型代入

$$
y=\frac{1}{1+e^{-{(w^Tx+b)}}}
$$

- 进而转为

$$
ln\frac{y}{1-y}=w^Tx+b
$$

- 其中,y是正例可能性，1-y是反例可能性

$$
\frac{y}{1-y}称为几率\\
ln\frac{y}{1-y}称为对数几率
$$

- 考虑后验概率形式

$$
ln\frac{p(y=1|x)}{p(y=0|x)}=w^Tx+b;\\
$$

​	那么
$$
p(y=1|x)=\frac{e^{w^Tx+b}}{1+e^{w^Tx+b}}\\
p(y=0|x)=\frac{1}{1+e^{w^Tx+b}}
$$

##### 损失函数

$cost = -y_i\log{(p)-(1-y)\log(1-p)}$

##### 极大似然估计

###### 通过若干次试验，观察其结果，利用试验结果得到某个参数值能够使样本出现的概率为最大，则称为极大似然估计

$$
l(\theta)=p(D|\theta)=p(x_1,x_2,...x_N|\theta)=\prod_ip(x_i|\theta)
$$

若θ能够使似然函数最大，那么就令θ为估计值

求解似然估计量步骤

1. 写出似然函数
2. 取对数，得到对数似然函数
3. 求导数
4. 解似然方程

##### 极大似然法求解 w , b 

- 对数似然函数

  ###### 将原来的类乘概率计算转化为累加

$$
l(\theta)=\sum_{i=1}^{m}{\ln p(y_i|x_i;w,b)}
$$

- 进行一些转化
  $$
  let . \;\beta=(w;b),\hat{x}=(x;1)\\
  so . \; w^Tx+b=\beta^Tx\\
  p_1(\hat{x};\beta)=p(y=1|\hat{x};\beta)=\frac{e^{\beta^T\hat{x_i}}}{1+e^{\beta^T\hat{x_i}}}\\
  p_0(\hat{x};\beta)=p(y=0|\hat{x};\beta)=\frac{1}{1+e^{\beta^T\hat{x_i}}}\\
  $$
  那么
  $$
  p(y_i|x_i;w,b)=y_ip_1(\hat{x_i};\beta)+(1-y_i)p_0(\hat{x_i};\beta)\\
  and \; y_i\in \{0,1\}\\
  so . \; \\
  if \;y_i=1,p(y_i|x_i;w,b)=p_1(\hat{x_i};\beta),\\
  y_i=0,p(y_i|x_i;w,b)=p_0(\hat{x_i};\beta)
  $$
  


- $$
  最大化\\
  l(\theta)=\sum_{i=1}^m(y_i\ln{P(y=1|x_i;w)+(1-y_i)\ln{(1-P(y=1|x_i;w))}})
  \\
  =\sum_{i=1}^m(y_i(\ln\frac{e^{\beta^T\hat{x_i}}}{1+e^{\beta^T\hat{x_i}}})+(1-y_i)\ln{\frac{1}{1+e^{\beta^T\hat{x_i}}}})
  \\
  =\sum_{i=1}^m(-y_i\beta^T\hat{x_i}+\ln(1+e^{\beta^T\hat{x_i}}))
  $$

-  ##### ##### 牛顿法,,

### 线性判别分析

###### LDA Linear Discriminant Analysis

###### 通过投影，降低到维度比较低的部分. 将高维的点投影到一条直线，使得点尽量的按类别区分

![img](https://upload-images.jianshu.io/upload_images/53611-32f117094c173b09.gif?imageMogr2/auto-orient/strip%7CimageView2/2/w/328/format/webp)

- 目标：不同类别之间距离远，同一类别距离近

- 区分二分类的直线
  $$
  y=w^Tx
  $$

- 类别$i $的中心点(均值向量)
  $$
  \mu_i=\frac1m(\sum_{x\in{D_i}} x)
  $$

- 协方差矩阵

$$
\sum_i
$$

当映射到新的空间时，样本中心为 $w^T\mu_i$ , 样本协方差为 $w^T\sum_iw$

##### 优化目标

```
我们期望：
  同类样本投影点的协方差尽可能小；不同样本之间尽可能远离
```

得到最优化($\max$)目标
$$
J=\frac{||w^T\mu_0-w^T\mu_1||_2^2}{w^T\sum_0w+w^T\sum_1w}\\
=\frac{w^T(\mu_0-\mu_1)(\mu_0-\mu_1)^Tw}{w^T(\sum_0+\sum_1)w}
$$
继续推导如下
$$
\frac{||w^T\mu_0-w^T\mu_1||_2^2}{w^T\sum_0w+w^T\sum_1w}=
\frac{||(w^T\mu_0-w^T\mu_1)^T||_2^2}{w^T(\sum_0+\sum_1)w}
\\
=\frac{(w^T\mu_0-w^T\mu_1)(w^T\mu_0-w^T\mu_1)^T}{w^T(\sum_0+\sum_1)w}\\
=\frac{w^T(\mu_0-\mu_1)(\mu_0-\mu_1)^Tw}{w^T(\sum_0+\sum_1)w}
$$

##### 广义瑞利商

###### 类内散度矩阵 $S_w=\sum_0+\sum_1$

###### 类间散度矩阵$S_b=(\mu_0-\mu_1)(\mu_0-\mu_1)^T$

那么我们的最大化目标为 $J=\frac{w^TS_bw}{w^TS_ww}$

称作 **广义瑞利商**

##### 最优化处理

```
由于分子分母都是w的二次项，所以最终和w长度无关
```

我们设 $w^TS_ww=1$ , 那么最优化等价于
$$
\min_{w}-w^TS_bw\\s.t.\;w^TS_ww=1
$$
如上可以通过拉格朗日乘子法解决
$$
L=-w^TS_bw+\lambda(w^TS_ww-1)
$$
我们对 $w$ 求偏导得到
$$
\frac{\partial L}{\partial w}=-\frac{\partial( w^TS_bw)}{\partial w}+\lambda\frac{\partial (w^TS_ww-1)}{\partial w}\\
=-(S_b+S_b^T)w+\lambda(S_w+S_w^T)w\\
=0
$$
得到 $S_bw = \lambda S_ww$

### 多分类学习

- `OvO` 一对一

1. 如果有n个类别，那么对他们两两配对，形成 n(n-1)/2 种配对方式，进行这么多次的二分类任务
2. 对于一个样本，看所有分类器中**投票数**最高者，即为其预测值

- `OvR` 一对多

1. 训练时，该标签为正例，其余均为反例。按照这个规律训练n个分类器
2. 最终测试时，看所有分类器中**得分**最高者，即为其预测值

- `MvM` 多对多

1. 若干个类作为正类，其他的位反类
2. 采用`纠错输出码`

### 类别不平衡问题

##### 背景

有时正例、反例的比例不一致，导致最终训练结果不平衡

##### 分类器决策规则

$$
if \;\frac{y}{1-y}>1\;,then\;Positive
$$

现在我们令
$$
m^+为正例个数，m^-为反例个数\\
那么观测几率为\frac{m^+}{m^-}\\
if \;\frac{y}{1-y}>\frac{m^+}{m^-}\;,then\;Positive
$$

##### 再缩放

$$
let.\;\frac{y'}{1-y'}=\frac{y}{1-y}\times\frac{m^-}{m^+}
$$

但是有时`观测几率不能推知真实几率`

有以下三种做法：

1. 欠采样：去除一些反例来让正反例比例接近
2. 过采样：增加正例比例
3. 阈值移动



## 习题

- 对原始空间 $X$ 做 $LDA$ 和在线性回归之后的新空间 $Y$ 中做 $LDA$ 的结果等价

原始空间中，

类内散度矩阵 $S_w=\sum{(\mu_i-\mu)(\mu_i-\mu)^T}$

累间散度矩阵 $S_b=\sum(x-\mu_i)(x-\mu_i)^T$

假设两个空间之间满足关系 $Y=B^TX$

那么新的空间中，

类内散度矩阵 $S_w'=\sum(B^T\mu_i-B^T\mu)(B^T\mu_i-B^T\mu)^T=\sum{B^T(\mu_i-\mu)B}$

类间散度矩阵 $S_b'=\sum B^T(x_i-\mu_i)(x_i-\mu_i)^TB$

那么可以得到


$$
J(w)=\frac{w^TS_bw}{w^TS_ww}\\
J(w)'=\frac{w^TB^TS_bBw}{w^TB^TS_wBw}
$$
令 $W=BW$ 即可

- 贝叶斯理论：当两类数据同先验，并且满足高斯分布且协方差相等，那么 $LDA$ 可以达到 **最优分类**

首先，贝叶斯最优分类器可以表示为
$$
h^*(x)=\arg\max{P(c|x)}
$$
在满足高斯分布的情况下
$$
\begin{align}
\arg\max P(c|x) & =\arg\max_{c\in Y} \ln f(c|x)\\
& =\arg\max_{c\in Y}  \ln(\frac1{(2\pi)^{1/2}|\sum|^{1/2}}\exp{(-\frac12(x-\mu_c)^T\Sigma	
^{-1}(x-\mu_c))})+\ln P(c)\\
& =\arg\max_{c\in Y}  -\frac12(x-\mu_c)^T\Sigma	
^{-1}(x-\mu_c) + \ln P(c)\\
& = \arg\max_{c\in Y} -\frac12(-x^T\Sigma^{-1}\mu_c-\mu_c^T\Sigma^{-1}x+\mu_c^T\Sigma^{-1}\mu_c) + \ln P(c)\\
& =\arg\max_{c\in Y} x^T\Sigma^{-1}\mu_c -\frac12\mu_c^T\Sigma^{-1}\mu_c + \ln P(c)
\end{align}
$$

- 设定 **决策边界** 

将 $\delta_c(x) = \arg\max_{c\in Y} x^T\Sigma^{-1}\mu_c -\frac12\mu_c^T\Sigma^{-1}\mu_c + \ln P(c)$ 记作线性判别函数

那么类别 $l , k$ 之间的决策边界为
$$
\{x:\delta_k(x)=\delta_l(x)\}\\
$$
得到如下等式
$$
\ln{\frac{P(k)}{P(l)}}-\frac12(\mu_k+\mu_l)^T\Sigma^{-1}(\mu_k-\mu_l)+x^T\Sigma^{-1}(\mu_k-\mu_l)=0
$$
如上是一个关于 $x$ 的线性函数，其对应的超平面法向量为
$$
w^*=\Sigma^{-1}(\mu_k-\mu_l)
$$
