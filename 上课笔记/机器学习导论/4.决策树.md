## 决策树

### 基本流程

#### 特征

- 每次都是对feature进行测试
- 下一层的测试，是基于上次决策的限定范围之内

#### 决策树训练过程

##### 伪码

```python
输入：训练集D = {(x1,y1) , (x2,y2) ,..., (xm,ym)}
	 属性A = {a1 , a2 ,..., ad}
	 
TreeGenerate(D,A):
	get node
	
    # 所有的标签一致
	if D 中样本全为类别C then
		let node to_be C 类叶子; 
		return;
	end if
    
    # 所有的标签可能不同，但是得分一致
    if A = None or D中样本在A上的得分相同 then
    	let node to_be D中样本数最多的类;
        return;
    end if
    
    从A中选择最优划分属性a*
    for 属性a* 的每一个可能取值 a`:
        为 node 生成一个分支；令Dv为a*上取值为a` 的样本子集;
        # 无样本 , 设定为父节点中样本最多的类
        if Dv is None then:
            let node to_be D中样本数最多的类;
        	return;
        else:
            以 TreeGenerate(Dv , A \ {a*})
   
```

以上算法关键在于：`从A中取出一个最优划分feature`

从而让分支节点的样本尽可能属于一个类 ， 即**纯度**上升

### 划分选择

##### 信息熵

度量样本集合的**纯度**
$$
设样本集合D中第k类样本的比例p_k(k=1,2,...,n)\\
则信息熵定义如下\\
Ent(D)=-\sum_{k=1}^mp_klog_2p_k\\
=-E[\log_2p_k]\\
=E[\log_2\frac1p_k]
$$

- 可见信息熵是 log 1/p 的数学期望
- 信息熵越小，则纯度越高

理解:
$$
\frac{1}{p_k}代表信息\\
E[\log_2\frac1p_k]表示信息在二进制表示下的期望
$$

##### 信息增益

我们令
$$
若离散属性a有V种取值\{a^1,a^2,...,a^V\},那么就有V个分支节点\\
其中第v个节点包含了D中所有在属性a上取值是a^v的样本，记作D^v
$$
按照如下定义：属性a在D样本上的信息增益
$$
Gain(D,a)=Ent(D)-\sum_{v=1}^V\frac{|D^v|}{|D|}Ent(D^v)
$$

一般，信息增益越大，表示利用a属性划分的纯度越大

也即
$$
a^*=\max_{a\in A}Gain(D,a)\\
该算法用于ID3决策树算法
$$

##### 增益率

考虑到信息增益准则会对`可取值数目较多的属性`有所偏好,**C4.5决策树算法**采用信息增益率来进行属性选择
$$
Gain\_ratio(D,a)=\frac{Gain(D,a)}{IV(a)}\\
其中IV(a)=-\sum_{v=1}^V\frac{|D^v|}{|D|}\log_2{\frac{|D^v|}{|D|}}称为属性a的固有值
$$
采用`启发式`：先从候选划分属性中找出**信息增益**高于平均水平的属性，在挑选**增益率**最高者

##### 基尼指数

- **CART决策树算法**使用基尼指数

$$
数据集D的纯度采用基尼值来进行度量\\
Gini(D)=\sum_{k=1}^m\sum_{k'\ne k}p_kp_{k'}
$$

直观反映了从D中取出两个样本，类别标记不一致的概率.

基尼值越小，纯度越大

- 基尼指数

$$
Gini\_index(D,a)=\sum_{v=1}^V\frac{|D^v|}{|D|}Gini(D)
$$

- 选择基尼指数**最小**的来进行划分

------



### 剪枝处理

- 主动去掉分支，解决过拟合问题

##### 预剪枝

- 基于贪心策略
- 主要想法：
  - 先计算划分前的计算正确率 p1
  - 计算：如果划分，那么划分后的正确率 p2
  - 若 p2 > p1 ， 可以进行划分；否则停止划分
  - 若可以继续划分，let  p1 = p2 ; 在分支处更新p2 . 重复上一步

##### 后剪枝

- 对每一个叶子节点上一层进行剪枝 , 如果正确率上升，则剪枝
- 时间开销比预剪枝大

------

### 连续值与缺失值

##### 连续值处理

- 连续属性离散化 (C4.5算法)
  - 二分法策略

$$
假定属性a在D上有n个不同取值\\
排序后为\{a^1,a^2,...,a^n\}\\
基于划分点t,可以将D分为D_t^-,D_t^+\\
对相邻属性取值a^i,a^{i+1}来说，t在[a^i,a^{i+1})上产生的划分相同\\
所以我们就考察n-1个候选划分点T_a=\{\frac{a^{i}+a^{i+1}}{2}|1\le i \le n-1\}
$$

如上所示，我们就把属性的相邻取值的中位点作为**候选划分点**
$$
Gain(D,a)=\max_{t\in{T_a}}Gain(D,a,t)\\
=\max_{t\in{T_a}}Ent(D)-\sum_{\lambda\in{\{+,-\}}}\frac{|D_t^{\lambda}|}{|D|}Ent(D_t^{\lambda})
$$

- 当前节点划分的属性a , 在子树中还将继续参与划分选择

##### 缺失值处理

问题:

1. 划分属性选择时，如果属性值缺失如何做
2. 给定划分属性，如果样本在该属性缺失值如何做

$$
\tilde{D}表示D中在属性a上没有缺失值的样本子集\\
a有V个可取值\{a^1,a^2,...,a^V\}\\
令\tilde{D^v}为属性a中取值为a^v的样本子集;\\
\tilde{D_k}表示第k类(k=1,2,...,n)样本子集
$$

我们为每一个样本x赋予权重w

作如下定义
$$
无缺失值样本所占比例\;\rho=\frac{\sum_{x\in \tilde{D}}\omega_x}{\sum_{x\in {D}}\omega_x}\\
无缺失值样本中第k类所占比例\;\tilde{p_k}=\frac{\sum_{x\in \tilde{D_k}}\omega_x}{\sum_{x\in {D}}\omega_x}\;,k\in\{1,2...,n\}\\
无缺失值样本中在a属性上取值a^v的样本所占比例\;\tilde{r_v}=\frac{\sum_{x\in \tilde{D^v}}\omega_x}{\sum_{x\in {D}}\omega_x}\;,v\in\{1,2...,V\}
$$
进而将信息增益推广为
$$
Gain(D,a)=\rho \times Gain(\tilde{D},a)\\
=\rho \times (Ent(\tilde{D})-\sum_{v=1}^V\tilde{r_v}Ent(\tilde{D^v}))\\
且Ent(\tilde{D^v}))=-\sum_{k=1}^n\tilde{p_k}\log_2{\tilde{p_k}}
$$

### 多变量决策树

- 单属性的划分，分类器的边界与**坐标轴平行**

- 非叶节点不再是针对某个属性进行划分，而是进行多个属性的线性组合t

$$
t=\sum_{i=1}^dw_ia_i
$$

