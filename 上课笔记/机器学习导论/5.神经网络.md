## 神经网络

### 神经元模型

##### 感知机与多层网络

`Perceptron`
$$
y_i=f(\sum{w_ix_i}-\theta)\\
其中\theta为阈值,f为激活函数
$$

### BP算法

#### 一个隐层

`误差逆传播算法`--error BackPropagation

![](https://img-blog.csdn.net/20170626215536538?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZGFpZ3VhbHU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

如上图所示
$$
b_h=f(\alpha_h-\delta_h),y_j=f(\beta_j-\theta_j)
$$
接下来叙述算法过程
$$
训练例(x_k,y_k),假定神经网络输出\hat{y_k}=(\hat{y}_1^k,\hat{y}_2^k,...\hat{y}_l^k)\\
其中\hat{y}_h^k=f(\beta_h-\theta_h)
$$

###### 均方误差

$$
E_k=\frac12\sum_{j=1}^l(\hat{y}_j^k-y_j^k)^2
$$

需要确定的参数：输入层到隐层的  d * q 个权重 ；隐层到输出层的 q * l 个权重 ；隐层 q 个阈值；输出层 l 个阈值

###### 基于梯度下降

- $$
  v=v+\Delta v
  $$

  

$$
\Delta w_{hj}=-\eta\frac{\partial E_k}{\partial w_{hj}} \;\;(1)\\
w_{hj}首先影响第j个输出层神经元的输入值\beta_j,再影响输出值\hat{y}_j^k,然后影响到E_k\\
Thus .\;\frac{\partial E_k}{\partial w_{hj}}=\frac{\partial E_k}{\partial \hat{y}_j^k}\times\frac{\partial\hat{y}_j^k }{\partial\beta_j}\times\frac{\partial \beta_j}{\partial w_{hj}}
$$

根据β定义
$$
\frac{\partial \beta_j}{\partial w_{hj}}=b_h\\
b_h为隐层的输出
$$
由于Sigmoid函数的导数满足
$$
f'(x)=f(x)(1-f(x))
$$
定义 g_i
$$
-\frac{\partial E_k}{\partial \hat{y}_j^k}\times\frac{\partial\hat{y}_j^k }{\partial\beta_j}\\
=-(\hat{y}_j^k-y_j^k)f'(\hat{y}_j^k)\\
=({y}_j^k-\hat y_j^k)\hat y_j^k(1-\hat y_j^k)\\
:=g_j
$$
带入 （1）得到：
$$
隐层到输出层的权重\Delta{w_{hj}}=\eta g_jb_h
$$
 同理可得
$$
输出层的阈值\Delta\theta_j=-\eta\frac{\partial E_k}{\partial\theta_j}\\
=-\eta g_j\\
=\eta(\hat{y}_j^k-y_j^k)\hat{y}_j^k(1-\hat{y}_j^k)
$$
对于输入层到隐层
$$
权重\Delta v_{ih}=\eta e_hx_i\\
隐层阈值\Delta\gamma_h=-\eta e_h 
$$

#### 多个隐层

##### 误差

$$
第l层第j个神经元的误差 \delta_j^l:=\frac{\partial C}{\partial z_j^l}\\
z_j^l为加权输入值\\
a_j^l为输出\\
w_{ij}^l为第l层第i个神经元到第l+1层第j个神经元的权重\\
b_j^l为激活阈值
$$

根据如上定义
$$
\delta_j^l=\frac{\partial C}{\partial a_k^l}\sigma'(z_j^l)=(a^l-y)\sigma'(z_j^l) \\ 其中\frac{\partial a_l^k}{\partial z_j^l}=\sigma'(z_j^l)
$$
进而

![](http://tensorfly.cn/special/deeplearning/images/tikz20.png)
$$
\frac{\partial C}{\partial b}=\delta , 即误差\delta和阈值(偏差)是一个意思\\
\frac{\partial C}{\partial w_{jk}^l}=a_k^{l-1}\delta_j^l 
\;\;\;故\frac{\partial C}{\partial w}=a_{in}\delta_{out}
$$
此外，相邻两层的误差满足如下关系
$$
\delta^l=((w^{l+1})^T\delta^{l+1}) \cdot\sigma'(z^l)
$$

##### 总结

![](http://tensorfly.cn/special/deeplearning/images/tikz21.png)

##### 分类

1. `标准BP算法`
   - 单个样本进行参数更新
   - 参数更新频率快
2. `累积BP算法`
   - 读取整个训练集之后进行参数更新
   - 更新速度可能比较慢

##### 防止过拟合

1. 早停
   - 若训练集误差降低但是验证机误差提高，停止训练
2. 正则化

$$
E=\lambda\sum_{k=1}^mE_k+(1-\lambda)\sum_iw_i^2
$$

